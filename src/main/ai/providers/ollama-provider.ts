import { Ollama } from '@langchain/ollama';
import { AIConfig, AIGenerationRequest, AIGenerationResult } from '@shared/types/ai';
import { BaseAIProvider, AITestResult, AIIntelligentTestResult } from './base-provider';

/**
 * Ollama ä¾›åº”å•†å®ç°
 */
export class OllamaProvider extends BaseAIProvider {
  
  /**
   * æµ‹è¯•é…ç½®è¿æ¥
   */
  async testConfig(config: AIConfig): Promise<AITestResult> {
    console.log(`æµ‹è¯• Ollama è¿æ¥ï¼Œä½¿ç”¨ baseURL: ${config.baseURL}`);
    
    try {
      const timeoutFetch = this.createTimeoutFetch(15000);
      const response = await timeoutFetch(`${config.baseURL}/api/tags`);
      
      if (response.ok) {
        const models = await this.getAvailableModels(config);
        return { success: true, models };
      } else {
        return { success: false, error: 'æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨' };
      }
    } catch (error: any) {
      console.error('Ollama è¿æ¥æµ‹è¯•å¤±è´¥:', error);
      const errorMessage = this.handleCommonError(error, 'ollama');
      return { success: false, error: errorMessage };
    }
  }

  /**
   * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  async getAvailableModels(config: AIConfig): Promise<string[]> {
    console.log(`è·å– Ollama æ¨¡å‹åˆ—è¡¨ - baseURL: ${config.baseURL}`);
    
    try {
      const url = `${config.baseURL}/api/tags`;
      console.log(`Ollama è¯·æ±‚URL: ${url}`);
      
      const timeoutFetch = this.createTimeoutFetch(10000);
      const response = await timeoutFetch(url);
      console.log(`Ollama å“åº”çŠ¶æ€: ${response.status}`);
      
      if (response.ok) {
        const data = await response.json();
        console.log('Ollama å“åº”æ•°æ®:', data);
        
        const models = data.models?.map((model: any) => model.name) || [];
        console.log(`Ollama è§£æå‡ºçš„æ¨¡å‹åˆ—è¡¨:`, models);
        
        // ç¡®ä¿è¿”å›å®Œæ•´çš„æ¨¡å‹åç§°ï¼ˆåŒ…å«ç‰ˆæœ¬åç¼€ï¼‰
        if (models.length > 0) {
          console.log('âœ… Ollama æ¨¡å‹åˆ—è¡¨è§£ææˆåŠŸï¼Œè¿”å›å®Œæ•´æ¨¡å‹åç§°');
          return models;
        } else {
          console.warn('âš ï¸ Ollama è¿”å›ç©ºæ¨¡å‹åˆ—è¡¨');
          return [];
        }
      }
    } catch (error) {
      console.error('è·å– Ollama æ¨¡å‹åˆ—è¡¨å¤±è´¥:', error);
      if (error instanceof Error && error.message?.includes('è¯·æ±‚è¶…æ—¶')) {
        console.warn('Ollama è¯·æ±‚è¶…æ—¶');
      }
    }
    return [];
  }

  /**
   * æ™ºèƒ½æµ‹è¯•
   */
  async intelligentTest(config: AIConfig): Promise<AIIntelligentTestResult> {
    if (!config.enabled) {
      return { success: false, error: 'é…ç½®å·²ç¦ç”¨' };
    }

    const model = config.defaultModel || config.customModel;
    if (!model) {
      return { success: false, error: 'æœªè®¾ç½®é»˜è®¤æ¨¡å‹' };
    }

    const testPrompt = 'è¯·ç”¨ä¸€å¥è¯ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚';

    try {
      const llm = new Ollama({
        baseUrl: config.baseURL,
        model: model
      });

      const response = await this.withTimeout(llm.invoke(testPrompt), 20000);
      const responseText = typeof response === 'string' ? response : (response as any)?.content || 'æµ‹è¯•æˆåŠŸ';

      return {
        success: true,
        response: responseText,
        inputPrompt: testPrompt
      };
    } catch (error: any) {
      console.error('Ollama æ™ºèƒ½æµ‹è¯•å¤±è´¥:', error);
      const errorMessage = this.handleCommonError(error, 'ollama');
      return { 
        success: false, 
        error: errorMessage,
        inputPrompt: testPrompt
      };
    }
  }

  /**
   * ç”Ÿæˆæç¤ºè¯
   */
  async generatePrompt(request: AIGenerationRequest & { config: AIConfig }): Promise<AIGenerationResult> {
    const { config } = request;
    
    if (!config.enabled) {
      throw new Error('é…ç½®å·²ç¦ç”¨');
    }

    const model = request.model || config.defaultModel || config.customModel;
    console.log('ğŸ” Ollama ç”Ÿæˆè°ƒè¯•ä¿¡æ¯:', {
      requestModel: request.model,
      configDefaultModel: config.defaultModel,
      configCustomModel: config.customModel,
      finalModel: model,
      configId: config.configId,
      configName: config.name
    });
    
    if (!model) {
      throw new Error('æœªæŒ‡å®šæ¨¡å‹');
    }

    const { systemPrompt, userPrompt } = this.buildPrompts(request, config);

    try {
      const llm = new Ollama({
        baseUrl: config.baseURL,
        model: model
      });

      const messages = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ];
      
      const response = await this.withSmartTimeout(
        llm.invoke(messages), 
        90000,
        5000,
        () => true
      );
      const generatedPrompt = typeof response === 'string' ? response : (response as any)?.content || '';

      return this.createGenerationResult(request, config, model, generatedPrompt);
    } catch (error: any) {
      console.error('Ollama ç”Ÿæˆæç¤ºè¯å¤±è´¥:', error);
      if (error.message?.includes('è¯·æ±‚è¶…æ—¶')) {
        throw new Error('ç”Ÿæˆè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æœåŠ¡çŠ¶æ€');
      }
      throw new Error(`ç”Ÿæˆå¤±è´¥: ${error.message}`);
    }
  }

  /**
   * æµå¼ç”Ÿæˆæç¤ºè¯
   */
  async generatePromptWithStream(
    request: AIGenerationRequest,
    config: AIConfig,
    onProgress: (charCount: number, partialContent?: string) => boolean,
    abortSignal?: AbortSignal
  ): Promise<AIGenerationResult> {
    const model = request.model || config.defaultModel || config.customModel;
    
    if (!model) {
      throw new Error('æœªæŒ‡å®šæ¨¡å‹');
    }

    if (!config.enabled) {
      throw new Error('é…ç½®å·²ç¦ç”¨');
    }

    const { systemPrompt, userPrompt } = this.buildPrompts(request, config);

    try {
      const llm = new Ollama({
        baseUrl: config.baseURL,
        model: model
      });

      const messages = [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ];
      
      let accumulatedContent = '';
      let lastContentUpdate = Date.now();
      let shouldStop = false;
      
      if (abortSignal?.aborted) {
        throw new Error('ç”Ÿæˆå·²è¢«ä¸­æ–­');
      }
      
      try {
        const streamPromise = (async () => {
          const stream = await llm.stream(messages);
          for await (const chunk of stream) {
            if (abortSignal?.aborted || shouldStop) {
              console.log('æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢æµå¼ç”Ÿæˆ');
              break;
            }
            
            const content = typeof chunk === 'string' ? chunk : (chunk as any)?.content;
            if (content) {
              accumulatedContent += content;
              lastContentUpdate = Date.now();
              
              const continueGeneration = onProgress(accumulatedContent.length, accumulatedContent);
              if (continueGeneration === false) {
                console.log('å‰ç«¯è¯·æ±‚åœæ­¢ç”Ÿæˆ');
                shouldStop = true;
                break;
              }
            }
          }
        })();
        
        await this.withSmartTimeout(
          streamPromise, 
          60000,
          2000,
          () => {
            if (shouldStop || abortSignal?.aborted) {
              return false;
            }
            
            const now = Date.now();
            const timeSinceLastUpdate = now - lastContentUpdate;
            return timeSinceLastUpdate < 5000;
          }
        );
        
      } catch (streamError) {
        if (shouldStop || abortSignal?.aborted) {
          throw new Error('ç”¨æˆ·ä¸­æ–­ç”Ÿæˆ');
        }
        
        console.warn('æµå¼ä¼ è¾“å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨:', streamError);
        if (streamError instanceof Error && streamError.message?.includes('è¯·æ±‚è¶…æ—¶')) {
          const now = Date.now();
          const timeSinceLastUpdate = now - lastContentUpdate;
          if (timeSinceLastUpdate > 10000 && accumulatedContent.length === 0) {
            throw new Error('ç”Ÿæˆè¶…æ—¶ï¼ŒAIæœåŠ¡å¯èƒ½æ— å“åº”ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æœåŠ¡çŠ¶æ€');
          } else if (timeSinceLastUpdate > 30000) {
            console.warn('æ£€æµ‹åˆ°ç”Ÿæˆå¯èƒ½å·²å®Œæˆï¼Œä½†è¿æ¥æœªæ­£å¸¸å…³é—­ï¼Œä½¿ç”¨å·²æœ‰å†…å®¹');
          } else {
            throw new Error(`ç”Ÿæˆä¸­æ–­ï¼Œå·²ç”Ÿæˆ${accumulatedContent.length}å­—ç¬¦ï¼Œè¯·é‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥`);
          }
        }
        
        if (accumulatedContent.length === 0) {
          if (abortSignal?.aborted || shouldStop) {
            throw new Error('ç”¨æˆ·ä¸­æ–­ç”Ÿæˆ');
          }
          
          const response = await this.withSmartTimeout(
            llm.invoke(messages), 
            90000,
            5000,
            () => true
          );
          accumulatedContent = typeof response === 'string' ? response : (response as any)?.content || '';
          
          const totalChars = accumulatedContent.length;
          for (let i = 0; i <= totalChars; i += Math.ceil(totalChars / 20)) {
            if (abortSignal?.aborted || shouldStop) {
              throw new Error('ç”¨æˆ·ä¸­æ–­ç”Ÿæˆ');
            }
            
            const currentCharCount = Math.min(i, totalChars);
            const partialContent = accumulatedContent.substring(0, currentCharCount);
            const continueGeneration = onProgress(currentCharCount, partialContent);
            if (continueGeneration === false) {
              throw new Error('ç”¨æˆ·ä¸­æ–­ç”Ÿæˆ');
            }
            await new Promise(resolve => setTimeout(resolve, 50));
          }
        }
      }

      if (shouldStop || abortSignal?.aborted) {
        throw new Error('ç”¨æˆ·ä¸­æ–­ç”Ÿæˆ');
      }

      return this.createGenerationResult(request, config, model, accumulatedContent);
    } catch (error: any) {
      console.error('Ollama æµå¼ç”Ÿæˆæç¤ºè¯å¤±è´¥:', error);
      if (error.message?.includes('è¯·æ±‚è¶…æ—¶')) {
        throw new Error('ç”Ÿæˆè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æœåŠ¡çŠ¶æ€');
      }
      throw new Error(`ç”Ÿæˆå¤±è´¥: ${error.message}`);
    }
  }


} 